{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64388fed",
   "metadata": {},
   "source": [
    "# Scraping automatizado\n",
    "\n",
    "En esta última lección vamos a programar un script que sea capaz de scrapear una página web de citas automáticamente, y no, no me refiero a una web de citas para conocer otras personas sino citas de diferentes autores, lo que en inglés se denomina *quote*.\n",
    "\n",
    "Se trata de una página preparada con fines educativos: https://quotes.toscrape.com/, dejo también [un enlace al archivo](https://web.archive.org/web/20220712030814/https://quotes.toscrape.com/) por si queda inaccesible.\n",
    "\n",
    "La web tiene diferentes páginas donde aparecen las citas célebres, con su texto, autor y unos tags de categoría. Nos permite buscar en el índice global página a página o directamente por etiquetas:\n",
    "\n",
    "![](docs/img01.png)\n",
    "\n",
    "## Requisitos\n",
    "\n",
    "El programa que vamos a crear constará de una clase `Citas` que recuperará todas las citas de la web y tendrá cuatro métodos estáticos:\n",
    "\n",
    "* `scrapear()`: Realizará el scrapeo de las citas en todas las páginas de la web.\n",
    "* `lista(limite)`: Imprimirá las primeras N citas de la lista, podemos cambiar el limite.\n",
    "* `etiqueta(nombre)`: Imprimirá las citas con una etiqueta concreta.\n",
    "* `autor(nombre)`: Imprimirá las citas de un autor concreto.\n",
    "\n",
    "Ejemplos de uso:\n",
    "\n",
    "```python\n",
    "Citas.scrapear()                # Scrapear todas las citas de la web\n",
    "Citas.lista()                   # Imprimir las primeras 10 citas (por defecto)\n",
    "Citas.lista(20)                 # Imprimir las primeras 20 citas\n",
    "Citas.etiqueta(\"love\")          # Citas con etiqueta 'love'\n",
    "Citas.autor(\"Albert Einstein\")  # Citas del autor 'Albert Einstein'\n",
    "```\n",
    "\n",
    "Si queréis os lo podéis tomar como un reto, aunque no es la finalidad de la lección, os dejo un par de consejos:\n",
    "\n",
    "* En la parte inferior hay un botón llamado *Next* para ir pasando a la siguiente página, podemos usarlo para iterar las páginas dinámicamente.\n",
    "* Scrapear una vez es mejor que scrapear dos veces, en ese sentido puede ser muy útil almacenar el contenido en un fichero para ahorrarnos múltiples peticiones web y el tiempo que eso conlleva.\n",
    "\n",
    "¡Vamos a por ello!\n",
    "\n",
    "## Pruebas de desarrollo\n",
    "\n",
    "Empecemos por lo más esencial, dada la portada de la página veamos si podemos extraer las citas con su respectivo autor y etiquetas.\n",
    "\n",
    "Si inspeccionamos la estructura de cada cita, se basa en una capa `div` con la clase `quote`, dentro un `span` con clase `text` contiene el texto, un tag `small` con clase `author` el autor y dentro de otra `div` con clase `tags` tenemos diferentes los tags en enlaces `a` con la clase `tag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63466ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "req = requests.get(\"https://quotes.toscrape.com\")\n",
    "soup = BeautifulSoup(req.text)\n",
    "\n",
    "# Buscamos las citas de la portada\n",
    "quotes_tags = soup.select(\"div.quote\")\n",
    "for quote_tag in quotes_tags:\n",
    "    # Buscamos el texto\n",
    "    print(quote_tag.select(\"span.text\")[0].getText())\n",
    "    # Buscamos el autor\n",
    "    print(quote_tag.select(\"small.author\")[0].getText())\n",
    "    # Buscamos las etiquetas\n",
    "    for tag in quote_tag.select(\"div.tags a.tag\"):\n",
    "        print(tag.getText(), end=\" \")\n",
    "    # Salto de línea para separar las citas\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a95fe",
   "metadata": {},
   "source": [
    "Bien, ya tenemos por donde empezar, podríamos adaptar este código a una función que a partir de una porción de la URL almacene mediante diccionarios las citas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_quotes(url=\"\"):\n",
    "    domain = \"https://quotes.toscrape.com\"\n",
    "    req = requests.get(f\"{domain}{url}\")\n",
    "    soup = BeautifulSoup(req.text)\n",
    "\n",
    "    # Lista para almacenar diccionarios que contendrán datos de las citas\n",
    "    quotes = []\n",
    "    # Buscamos las citas de la portada\n",
    "    quotes_tags = soup.select(\"div.quote\")\n",
    "    for quote_tag in quotes_tags:\n",
    "        # Creamos un diccionario vacío\n",
    "        quote = {}\n",
    "        # Almacenamos los diferentes campos en el diccinario\n",
    "        quote['text'] = quote_tag.select(\"span.text\")[0].getText()\n",
    "        quote['author'] = quote_tag.select(\"small.author\")[0].getText()\n",
    "        quote['tags'] = []\n",
    "        for tag in quote_tag.select(\"div.tags a.tag\"):\n",
    "            quote['tags'].append(tag.getText())\n",
    "        # Añadimos el diccionario con la cita a la lista\n",
    "        quotes.append(quote)\n",
    "    # Devolvemos las citas scrapeadas\n",
    "    return quotes\n",
    "\n",
    "quotes = scrap_quotes()\n",
    "\n",
    "for quote in quotes:\n",
    "    print(quote[\"text\"])\n",
    "    print(quote[\"author\"])\n",
    "    for tag in quote[\"tags\"]:\n",
    "        print(tag, end=\" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf4481",
   "metadata": {},
   "source": [
    "La clave es utilizar nuestra función de forma recursiva detectando si la página tiene el enlace **Next** y cargando la siguiente página de manera que podamos. Veamos cómo extraer el enlace con la siguiente página si la hay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"https://quotes.toscrape.com\"\n",
    "req = requests.get(domain)\n",
    "soup = BeautifulSoup(req.text)\n",
    "\n",
    "# Buscamos el enlace en el tag li con clase next\n",
    "link_tag = soup.select(\"li.next a\")\n",
    "# Si hay como mínimo un enlace extraemos su href relativo sumado al dominio\n",
    "if len(link_tag) > 0:\n",
    "    next_url = link_tag[0]['href']\n",
    "    print(next_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8d0c8",
   "metadata": {},
   "source": [
    "Podemos integrar este código en nuestra función `scrap_quotes` para devolver no solo las citas de la página, sino también si hay una página siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43afbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_quotes(url=\"\"):\n",
    "    domain = \"https://quotes.toscrape.com\"\n",
    "    req = requests.get(f\"{domain}{url}\")\n",
    "    soup = BeautifulSoup(req.text)\n",
    "\n",
    "    # Lista para almacenar diccionarios que contendrán datos de las citas\n",
    "    quotes = []\n",
    "    # Buscamos las citas de la portada\n",
    "    quotes_tags = soup.select(\"div.quote\")\n",
    "    for quote_tag in quotes_tags:\n",
    "        # Creamos un diccionario vacío\n",
    "        quote = {}\n",
    "        # Almacenamos los diferentes campos en el diccinario\n",
    "        quote['text'] = quote_tag.select(\"span.text\")[0].getText()\n",
    "        quote['author'] = quote_tag.select(\"small.author\")[0].getText()\n",
    "        quote['tags'] = []\n",
    "        for tag in quote_tag.select(\"div.tags a.tag\"):\n",
    "            quote['tags'].append(tag.getText())\n",
    "        # Añadimos el diccionario con la cita a la lista\n",
    "        quotes.append(quote)\n",
    "        \n",
    "    # Buscamos el enlace en el tag li con clase next\n",
    "    next_url = None\n",
    "    link_tag = soup.select(\"li.next a\")\n",
    "    # Si hay como mínimo un enlace extraemos su href relativo sumado al dominio\n",
    "    if len(link_tag) > 0:\n",
    "        next_url = link_tag[0]['href']\n",
    "    \n",
    "    # Imprimiros un mensaje informativo\n",
    "    print(f\"Página {domain}{url}, {len(quotes)} citas scrapeadas.\")\n",
    "    \n",
    "    # Devolvemos las citas scrapeadas y la siguiente página, que puede ser None\n",
    "    return quotes, next_url\n",
    "\n",
    "quotes, next_url = scrap_quotes()\n",
    "\n",
    "print() # Espacio en blanco\n",
    "print(next_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93972d2",
   "metadata": {},
   "source": [
    "Ahora se viene la parte interesante, vamos a implementar una función que scrapee todas las páginas mientras haya una siguente o, alternativamente, podemos establecer un límite para optimizar el proceso y no saturar al servidor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e73e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_site(limit=2):\n",
    "    # Definimos una lista global para almacenar todas las citas\n",
    "    all_quotes = []\n",
    "    # Definimos la siguiente URL que irá cambiando (inicialmente es el dominio raíz)\n",
    "    next_url = \"\" \n",
    "    # Iniciamos un bucle infinito\n",
    "    while 1:\n",
    "        # Scrapeamos la página, guardamos las citas scrapeadas y la siguiente página\n",
    "        quotes, next_url = scrap_quotes(next_url)\n",
    "        # Añadimos las citas scrapeadas a la lista global\n",
    "        all_quotes += quotes\n",
    "        # Restamos 1 al limite \n",
    "        limit -= 1\n",
    "        # Si lo superamos o no hay siguiente página finalizamos la función\n",
    "        if limit == 0 or next_url == None:\n",
    "            # Finalizamos la función\n",
    "            return all_quotes\n",
    "\n",
    "quotes = scrap_site()\n",
    "\n",
    "print() # Espacio en blanco\n",
    "for quote in quotes:\n",
    "    print(quote[\"text\"])\n",
    "    print(quote[\"author\"])\n",
    "    for tag in quote[\"tags\"]:\n",
    "        print(tag, end=\" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97745d99",
   "metadata": {},
   "source": [
    "Ahí la tenemos, una función capaz de scrapear todas las citas de la página por defecto limitado a 2 páginas.\n",
    "\n",
    "## Implementando la clase Citas\n",
    "\n",
    "Vamos a ponernos con la clase `Citas` y el método `scrapear` pero siguiendo el consejo que os dí de crear un fichero donde almacenar todas las citas.\n",
    "\n",
    "### Guardado en fichero\n",
    "\n",
    "Solo generaremos el fichero si ejecutamos el método `scrapear`, los demás métodos `lista`, `etiqueta` y `autor` analizarán el contenido del fichero volcado en la memoria, pero nunca scrapearán nada directamente.\n",
    "\n",
    "Después de valorarlo he decidido utilizar un CSV. Lo único que nos dará algún problema es guardar una lista como un campo del registro, pero podemos recuperarla evaluándola de nuevo, ya veréis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class Citas:\n",
    "    \n",
    "    # Variable de clase para almacenar las citas en la memoria\n",
    "    quotes = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def scrapear():\n",
    "        # Scrapeamos todas las citas, ponemos un límite pequeño para hacer pruebas\n",
    "        Citas.quotes = scrap_site(limit=2)\n",
    "        # Guardamos las citas scrapeadas en un fichero CSV volcándolas de la lista de dicts\n",
    "        with open(\"quotes.csv\", \"w\") as file:\n",
    "            # Definimos el objeto para escribir con las cabeceras de los campos \n",
    "            writer = csv.DictWriter(file, fieldnames=[\"text\", \"author\", \"tags\"])\n",
    "            # Escribimos las cabeceras\n",
    "            writer.writeheader()\n",
    "            # Escribimos cada cita en la memoria en el fichero\n",
    "            for quote in Citas.quotes:\n",
    "                writer.writerow(quote)\n",
    "            \n",
    "Citas.scrapear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b7116",
   "metadata": {},
   "source": [
    "En este punto deberíamos tener un fichero `quotes.csv` con todas las citas, lo que podríamos hacer es cargar en la memoria todas las citas del fichero en caso de que éste exista. De paso podemos implementar el método `lista` para consultarlas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b71539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "class Citas:\n",
    "    \n",
    "    # Variable de clase para almacenar las citas en la memoria\n",
    "    quotes = []\n",
    "    \n",
    "    # Recuperamos las citas en la memoria si existe el fichero quotes.csv\n",
    "    if os.path.exists(\"quotes.csv\"):\n",
    "        with open(\"quotes.csv\", \"r\") as file:\n",
    "            data = csv.DictReader(file)\n",
    "            for quote in data:\n",
    "                # La lista es una cadena, hay que reevaluarla\n",
    "                quote['tags'] = eval(quote['tags'])\n",
    "                quotes.append(quote)\n",
    "    \n",
    "    @staticmethod\n",
    "    def scrapear():\n",
    "        # Scrapeamos todas las citas, ponemos un límite pequeño para hacer pruebas\n",
    "        Citas.quotes = scrap_site(limit=2)\n",
    "        # Guardamos las citas scrapeadas en un fichero CSV volcándolas de la lista de dicts\n",
    "        with open(\"quotes.csv\", \"w\") as file:\n",
    "            # Definimos el objeto para escribir con las cabeceras de los campos \n",
    "            writer = csv.DictWriter(file, fieldnames=[\"text\", \"author\", \"tags\"])\n",
    "            # Escribimos las cabeceras\n",
    "            writer.writeheader()\n",
    "            # Escribimos cada cita en la memoria en el fichero\n",
    "            for quote in Citas.quotes:\n",
    "                writer.writerow(quote)\n",
    "            \n",
    "    @staticmethod\n",
    "    def listar(limite=10):\n",
    "        for quote in Citas.quotes[:limite]:\n",
    "            print(quote[\"text\"])\n",
    "            print(quote[\"author\"])\n",
    "            for tag in quote[\"tags\"]:\n",
    "                print(tag, end=\" \")\n",
    "            print(\"\\n\")\n",
    "            \n",
    "Citas.listar(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b83103b",
   "metadata": {},
   "source": [
    "### Filtro por etiqueta y autor\n",
    "\n",
    "Ya solo nos falta implementar los métodos de filtrado por etiqueta y autor, es muy fácil porque solo tenemos que recorrer las citas y comprobar si concuerdan con los valores que pasamos a los métodos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4980f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "class Citas:\n",
    "    \n",
    "    # Variable de clase para almacenar las citas en la memoria\n",
    "    quotes = []\n",
    "    \n",
    "    # Recuperamos las citas en la memoria si existe el fichero quotes.csv\n",
    "    if os.path.exists(\"quotes.csv\"):\n",
    "        with open(\"quotes.csv\", \"r\") as file:\n",
    "            data = csv.DictReader(file)\n",
    "            for quote in data:\n",
    "                # La lista es una cadena, hay que reevaluarla\n",
    "                quote['tags'] = eval(quote['tags'])\n",
    "                quotes.append(quote)\n",
    "    \n",
    "    @staticmethod\n",
    "    def scrapear():\n",
    "        # Scrapeamos todas las citas, ponemos un límite pequeño para hacer pruebas\n",
    "        Citas.quotes = scrap_site(limit=2)\n",
    "        # Guardamos las citas scrapeadas en un fichero CSV volcándolas de la lista de dicts\n",
    "        with open(\"quotes.csv\", \"w\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=[\"text\", \"author\", \"tags\"])\n",
    "            writer.writeheader()\n",
    "            for quote in Citas.quotes:\n",
    "                writer.writerow(quote)\n",
    "            \n",
    "    @staticmethod\n",
    "    def listar(limite=10):\n",
    "        for quote in Citas.quotes[:limite]:\n",
    "            print(quote[\"text\"])\n",
    "            print(quote[\"author\"])\n",
    "            for tag in quote[\"tags\"]:\n",
    "                print(tag, end=\" \")\n",
    "            print(\"\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def etiqueta(nombre=\"\"):\n",
    "        for quote in Citas.quotes:\n",
    "            if nombre in quote[\"tags\"]:\n",
    "                print(quote[\"text\"])\n",
    "                print(quote[\"author\"])\n",
    "                for tag in quote[\"tags\"]:\n",
    "                    print(tag, end=\" \")\n",
    "                print(\"\\n\")\n",
    "                \n",
    "    @staticmethod\n",
    "    def autor(nombre=\"\"):\n",
    "        for quote in Citas.quotes:\n",
    "            if nombre == quote[\"author\"]:\n",
    "                print(quote[\"text\"])\n",
    "                print(quote[\"author\"])\n",
    "                for tag in quote[\"tags\"]:\n",
    "                    print(tag, end=\" \")\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c830e4",
   "metadata": {},
   "source": [
    "Veamos cuantas citas tenemos con el tag **love**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281adbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Citas.etiqueta(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80399b94",
   "metadata": {},
   "source": [
    "Y del autor **Albert Einstein**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12479f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Citas.autor(\"Albert Einstein\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cfa125",
   "metadata": {},
   "source": [
    "## Scrapeo de la web completa\n",
    "\n",
    "El programa está limitado a las 2 primeras páginas, voy a reescribir el código con un límite muy grande que garantice un scrapeo completo de la web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f9418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def scrap_quotes(url=\"\"):\n",
    "    domain = \"https://quotes.toscrape.com\"\n",
    "    req = requests.get(f\"{domain}{url}\")\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    \n",
    "    quotes = []\n",
    "    quotes_tags = soup.select(\"div.quote\")\n",
    "    for quote_tag in quotes_tags:\n",
    "        quote = {}\n",
    "        quote['text'] = quote_tag.select(\"span.text\")[0].getText()\n",
    "        quote['author'] = quote_tag.select(\"small.author\")[0].getText()\n",
    "        quote['tags'] = []\n",
    "        for tag in quote_tag.select(\"div.tags a.tag\"):\n",
    "            quote['tags'].append(tag.getText())\n",
    "        quotes.append(quote)\n",
    "        \n",
    "    next_url = None\n",
    "    link_tag = soup.select(\"li.next a\")\n",
    "    if len(link_tag) > 0:\n",
    "        next_url = link_tag[0]['href']\n",
    "        \n",
    "    print(f\"Página {domain}{url}, {len(quotes)} citas scrapeadas.\")\n",
    "        \n",
    "    return quotes, next_url\n",
    "\n",
    "\n",
    "def scrap_site(limit=2):\n",
    "    all_quotes = []\n",
    "    next_url = \"\" \n",
    "    while 1:\n",
    "        quotes, next_url = scrap_quotes(next_url)\n",
    "        all_quotes += quotes\n",
    "        limit -= 1\n",
    "        if limit == 0 or next_url == None:\n",
    "            return all_quotes\n",
    "\n",
    "        \n",
    "class Citas:\n",
    "    quotes = []\n",
    "    \n",
    "    if os.path.exists(\"quotes.csv\"):\n",
    "        with open(\"quotes.csv\", \"r\") as file:\n",
    "            data = csv.DictReader(file)\n",
    "            for quote in data:\n",
    "                quote['tags'] = eval(quote['tags'])\n",
    "                quotes.append(quote)\n",
    "    \n",
    "    @staticmethod\n",
    "    def scrapear():\n",
    "        Citas.quotes = scrap_site(limit=99) # <--- LIMITE MUY GRANDE\n",
    "        with open(\"quotes.csv\", \"w\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=[\"text\", \"author\", \"tags\"])\n",
    "            writer.writeheader()\n",
    "            for quote in Citas.quotes:\n",
    "                writer.writerow(quote)\n",
    "            \n",
    "    @staticmethod\n",
    "    def listar(limite=10):\n",
    "        for quote in Citas.quotes[:limite]:\n",
    "            print(quote[\"text\"])\n",
    "            print(quote[\"author\"])\n",
    "            for tag in quote[\"tags\"]:\n",
    "                print(tag, end=\" \")\n",
    "            print(\"\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def etiqueta(nombre=\"\"):\n",
    "        for quote in Citas.quotes:\n",
    "            if nombre in quote[\"tags\"]:\n",
    "                print(quote[\"text\"])\n",
    "                print(quote[\"author\"])\n",
    "                for tag in quote[\"tags\"]:\n",
    "                    print(tag, end=\" \")\n",
    "                print(\"\\n\")\n",
    "                \n",
    "    @staticmethod\n",
    "    def autor(nombre=\"\"):\n",
    "        for quote in Citas.quotes:\n",
    "            if nombre == quote[\"author\"]:\n",
    "                print(quote[\"text\"])\n",
    "                print(quote[\"author\"])\n",
    "                for tag in quote[\"tags\"]:\n",
    "                    print(tag, end=\" \")\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cad9b3",
   "metadata": {},
   "source": [
    "Vamos a ejecutar el scrapeo completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b54b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Citas.scrapear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b72edb8",
   "metadata": {},
   "source": [
    "Veamos cuantas citas encuentra ahora con el tag **love**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Citas.etiqueta(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67abfce",
   "metadata": {},
   "source": [
    "Y cuantas del autor **Albert Einstein**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d30296",
   "metadata": {},
   "outputs": [],
   "source": [
    "Citas.autor(\"Albert Einstein\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861f3395",
   "metadata": {},
   "source": [
    "Parece que todo funciona correctamente y podemos hacer tantas consultas como queramos sin repetir una y otra vez el proceso de scrapeo. En la práctica podríamos configurar un script que scrapee la página una vez al día para tener el fichero CSV sincronizado.\n",
    "\n",
    "En cualquier caso con esto acabamos este ejemplo y también la sección, espero que hayáis aprendido mucho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ca2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
